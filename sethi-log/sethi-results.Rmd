---
output:
  html_document: 
    keep_md: yes
    variant: markdown_github
---  

```{r}
library(purrr)
library(ggplot2)
library(dplyr)
library(appl)
```


```{r}
# alphas <- original_alphas
# models <- original_models

meta <- meta_from_log(data.frame(model = "ricker"))

alphas <- alphas_from_log(meta)
meta$sigma_g <- as.numeric(meta$sigma_g)
models <- models_from_log(meta)


states <- seq(meta[1,]$min_state, meta[1,]$max_state, len=meta[1,]$n_states)
actions <- seq(meta[1,]$min_action, meta[1,]$max_action, len=meta[1,]$n_action)
discount <- meta[1,]$discount
```


## Det policy

```{r}
f <- f_from_log(meta)[[1]]

S_star <- optimize(function(x) x / discount - f(x,0), c(min(states),max(states)))$minimum
h <- pmax(states - S_star,  0)
policy <- sapply(h, function(h) which.min((abs(h - actions))))
det <- data.frame(policy, value = 1:length(states), state = 1:length(states))
```


## Convergence testing

```{r}
intermediate_policies <- function(meta, log_dir = "."){
  lapply(1:dim(meta)[[1]], function(i){
    id <- meta[i,"id"]
    lapply(list.files(log_dir, paste0(id, ".*\\.policy")), function(file){
      appl:::read_policyx(paste0(log_dir, "/", file))
    })
  })
}

df1 <- 
purrr::map_df(1:length(models), function(j){
  alphas <- intermediate_policies(meta)[[j]]
  m <- models[[j]]
  purrr::map_df(1:length(alphas), function(i)
    compute_policy(alphas[[i]], m$transition, m$observation, m$reward),
    .id = "intermediate") 
}, .id = "model")

df1 %>% 
  ggplot(aes(states[state], states[state] - actions[policy], col=intermediate)) + 
  geom_line() + 
  facet_wrap(~model, scales = "free") + 
  geom_line(data = det, col="black")


```

## Explore POMDP policy


```{r}
df <- purrr::map_df(1:length(models), function(i)
  compute_policy(alphas[[i]], models[[i]]$transition, models[[i]]$observation, models[[i]]$reward),
  .id = "model")

## Join to metadata table
meta$model <- as.character(1:length(models))
df <- dplyr::left_join(df, meta, by = "model")
```



```{r}
df %>% filter(noise == "lognormal") %>% 
  ggplot(aes(states[state], states[state] - actions[policy], col = as.factor(r))) +
  geom_line() +
  facet_grid(sigma_m ~ sigma_g, scales = "free") +
  geom_line(data = det, col="black")
```



```{r}
df %>% filter(noise == "uniform") %>% 
  ggplot(aes(states[state], states[state] - actions[policy], col = as.factor(r))) +
  geom_line() +
  facet_grid(sigma_m ~ sigma_g, scales = "free") +
  geom_line(data = det, col="black")
```



## Simulation

```{r}
s1 <- meta_from_log(data.frame(r = 1, sigma_m = 0.5, sigma_g = 0.5, noise = "lognormal"))
s1$sigma_g <- as.numeric(s1$sigma_g)
alpha <- alphas_from_log(s1)[[1]]
m <- models_from_log(s1)[[1]]

```

Simulation in which belief over states is updated in each time step, used to determine a new policy, which is then used to choose the best action given the most recent observation.

```{r}
sim <- purrr::map_df(1:50, function(i) appl::sim_pomdp(m$transition, m$observation, m$reward, discount, x0 = 50, Tmax = 100, alpha = alpha)$df, .id = "rep")
```


Simulation in which the policy is not updated based on the most recent belief over states, but remains the policy given a uniform prior over belief states. 

```{r}
s <- compute_policy(alpha,m$transition, m$observation, m$reward)
sim2 <- purrr::map_df(1:50, function(i) mdplearning::mdp_planning(m$transition, m$reward, discount, x0 = 50, Tmax = 100, observation = m$observation, policy = s$policy), .id = "rep")
```



```{r}
sim3 <- purrr::map_df(1:50, function(i) mdplearning::mdp_planning(m$transition, m$reward, discount, x0 = 50, Tmax = 100, observation = m$observation, policy = det$policy), .id = "rep")
```


```{r}
df <- bind_rows(pomdp = sim, static = sim2, det = sim3, .id = "scenario")
df %>% group_by(scenario, rep) %>% summarise(value = sum(value)) %>%  group_by(scenario) %>% summarise(mean(value))

df %>% group_by(scenario, time) %>% summarise(value = mean(value), state = mean(state)) %>%
ggplot(aes(time, state, col = scenario)) + geom_line()
```


## Small noise


```{r}
s1 <- meta_from_log(data.frame(r = 1, sigma_m = 0.5, sigma_g = 0.1, noise = "lognormal"))
alpha <- alphas_from_log(s1)[[1]]

s1$sigma_g <- as.numeric(s1$sigma_g)

m <- models_from_log(s1)[[1]]

sim <- purrr::map_df(1:50, function(i) appl::sim_pomdp(m$transition, m$observation, m$reward, discount, x0 = 50, Tmax = 100, alpha = alpha)$df, .id = "rep")
s <- compute_policy(alpha,m$transition, m$observation, m$reward)
sim2 <- purrr::map_df(1:50, function(i) mdplearning::mdp_planning(m$transition, m$reward, discount, x0 = 50, Tmax = 100, observation = m$observation, policy = s$policy), .id = "rep")
sim3 <- purrr::map_df(1:50, function(i) mdplearning::mdp_planning(m$transition, m$reward, discount, x0 = 50, Tmax = 100, observation = m$observation, policy = det$policy), .id = "rep")

df <- bind_rows(pomdp = sim, static = sim2, det = sim3, .id = "scenario")
df %>% group_by(scenario, rep) %>% summarise(value = sum(value)) %>%  group_by(scenario) %>% summarise(mean(value))

df %>% group_by(scenario, time) %>% summarise(value = mean(value), state = mean(state)) %>%
ggplot(aes(time, state, col = scenario)) + geom_line()
```

