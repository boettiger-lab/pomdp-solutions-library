---
output:
  html_document: 
    keep_md: yes
    variant: markdown_github
---  

```{r}
library("appl")
library("dplyr")
library("parallel")
mc.cores = 26

log_dir = "."
states <- seq(0, 1, len=50)
actions <- states  # Vector of actions: harvest
obs <- states

K = 0.33
discount = 0.95

## Usual assumption at the moment for reward fn
reward_fn <- function(x,h) pmin(x,h)

```

```{r}

## Ricker Models
vars <- expand.grid(r = c(0.5, 1), sigma_m = c(0.1, 0.5), sigma_g = c(0.1, 0.5), noise = c("lognormal", "uniform"))
fixed <- data.frame( K = K,
                     C = NA,
                     model = "ricker",
                     precision = 0.0000001,
                     timeout = 20000,
                     timeInterval = 5000,
                     max_state = max(states), max_obs = max(obs), max_action = max(actions), 
                     min_state = min(states), min_obs = min(obs), min_action = min(actions),
                     cost = "none", 
                     beta = NA)
set1 <- data.frame(vars, fixed)

## Allen, B-H models
vars <- expand.grid(sigma_m = c(0.1, 0.5), sigma_g = c(0.1, 0.5), model = c("allen", "bh"))
fixed <- data.frame( K = K,
                     r = 1,
                     C = 0.05,
                     noise = "lognormal",
                     precision = 0.0000001,
                     timeout = 20000,
                     timeInterval = 5000,
                     max_state = max(states), max_obs = max(obs), max_action = max(actions), 
                     min_state = min(states), min_obs = min(obs), min_action = min(actions),
                     cost = "none",
                     beta = NA)
set2 <- data.frame(vars, fixed)


## Alternate reward fns
vars <- expand.grid(cost = c("linear", "quadaratic"))
fixed <- data.frame( K = K,
                     r = 1,
                     C = NA,
                     sigma_g = 0.1,
                     sigma_m = 0.5,
                     noise = "lognormal",
                     model = "ricker",
                     precision = 0.0000001,
                     timeout = 20000,
                     timeInterval = 5000,
                     max_state = max(states), max_obs = max(obs), max_action = max(actions), 
                     min_state = min(states), min_obs = min(obs), min_action = min(actions),
                     beta = 0.01)
set3 <- data.frame(vars, fixed)

pars <- bind_rows(set1, set2, set3)

```

```{r}
## Compute alphas for the above examples
models <- mclapply(1:dim(pars)[1], function(i){
  ## Select the model
  f <- switch(pars[i, "model"], 
              allen = appl:::allen(pars[i, "r"], pars[i, "K"], pars[i, "C"]),
              ricker = appl:::ricker(pars[i, "r"], pars[i, "K"]),
              bh = appl:::bh(pars[i, "r"], pars[i, "K"])
  )
  
  reward_fn <- switch(pars[i, "cost"],
                   none = function(x,h) pmin(x,h),
                   linear = function(x,h) pmin(x,h) - pars[i, "beta"] * h,
                   quadratic = function(x,h) pmin(x,h) - beta * h ^ 2
                   )
  
  ## Compute matrices
  fisheries_matrices(states, actions, obs, 
                     reward_fn = reward_fn, 
                     f = f, 
                     sigma_g = pars[i, "sigma_g"], 
                     sigma_m  = pars[i, "sigma_m"],
                     noise = as.character(pars[i, "noise"]))
},
mc.cores = mc.cores)

```



```{r}  
alphas <- mclapply(1:length(models), function(i){
         sarsop(models[[i]]$transition,
                models[[i]]$observation,
                models[[i]]$reward,
                discount = discount,
                precision = pars[i, "precision"],
                timeout = pars[i, "timeout"],
                timeInterval = pars[i, "timeInterval"],
                log_dir = log_dir,
                log_data = pars[i,])},
         mc.cores = mc.cores)
```
